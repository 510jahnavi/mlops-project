{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class DataBiasDetection:\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        \"\"\"Initialize with dataset.\"\"\"\n",
    "        self.data = data\n",
    "        self.bias_report = {}\n",
    "\n",
    "    def data_slicing(self, slice_cols: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Slice data based on unique values in specified columns.\"\"\"\n",
    "        sliced_data = {}\n",
    "        for col in slice_cols:\n",
    "            unique_vals = self.data[col].unique()\n",
    "            for val in unique_vals:\n",
    "                slice_name = f\"{col}_{val}\"\n",
    "                sliced_data[slice_name] = self.data[self.data[col] == val]\n",
    "                logging.info(f\"Data slice created: {slice_name} with {len(sliced_data[slice_name])} rows.\")\n",
    "        return sliced_data\n",
    "\n",
    "    def calculate_statistics(self, sliced_data: Dict[str, pd.DataFrame], feature_col: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate mean statistics for each data slice.\"\"\"\n",
    "        slice_statistics = {}\n",
    "        for slice_name, df_slice in sliced_data.items():\n",
    "            mean_value = df_slice[feature_col].mean()\n",
    "            slice_statistics[slice_name] = mean_value\n",
    "            logging.info(f\"Mean {feature_col} for slice {slice_name}: {mean_value:.2f}\")\n",
    "        return slice_statistics\n",
    "\n",
    "    def detect_bias(self, slice_statistics: Dict[str, float], threshold_ratio: float = 0.2) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Detect bias by identifying slices with significant mean deviation.\n",
    "        Returns the biased slices and their mean values if bias is detected.\n",
    "        \"\"\"\n",
    "        overall_mean = np.mean(list(slice_statistics.values()))\n",
    "        biased_slices = [\n",
    "            (slice_name, mean_value) for slice_name, mean_value in slice_statistics.items()\n",
    "            if abs(mean_value - overall_mean) > threshold_ratio * overall_mean\n",
    "        ]\n",
    "        \n",
    "        # Log bias detection\n",
    "        if biased_slices:\n",
    "            logging.warning(f\"Bias detected in slices: {[slice[0] for slice in biased_slices]}\")\n",
    "            self.bias_report['biased_slices'] = biased_slices\n",
    "        else:\n",
    "            logging.info(\"No significant bias detected.\")\n",
    "            self.bias_report['biased_slices'] = []\n",
    "\n",
    "        return biased_slices\n",
    "\n",
    "    def document_bias_report(self) -> None:\n",
    "        \"\"\"Log and document bias detection results.\"\"\"\n",
    "        logging.info(\"Bias Report:\")\n",
    "        for key, value in self.bias_report.items():\n",
    "            logging.info(f\"{key}: {value}\")\n",
    "\n",
    "    def mitigate_bias_resample_with_imputation(self, slice_cols: List[str], date_col: str, feature_cols: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Mitigate bias by re-sampling underrepresented slices with rolling average imputation for missing dates.\"\"\"\n",
    "        max_count = max(len(self.data[self.data[col] == val])\n",
    "                        for col in slice_cols for val in self.data[col].unique())\n",
    "        \n",
    "        print(self.data.columns)\n",
    "        \n",
    "        resampled_data = pd.DataFrame()\n",
    "        date_range = pd.date_range(start=self.data[date_col].min(), end=self.data[date_col].max(), freq='D')\n",
    "\n",
    "        for col in slice_cols:\n",
    "            for val in self.data[col].unique():\n",
    "                # Create a subset for each unique value in the slice column\n",
    "                subset = self.data[self.data[col] == val].set_index(date_col)\n",
    "                \n",
    "                # Reindex to include all dates in the range and fill missing feature columns using rolling average\n",
    "\n",
    "                subset = subset.reindex(date_range).sort_index()\n",
    "                subset[slice_cols] = val  # Ensure the slice column retains its value\n",
    "                for feature in feature_cols:\n",
    "                    subset[feature] = subset[feature].fillna(subset[feature].rolling(window=3, min_periods=1).mean())\n",
    "\n",
    "                # Ensure we have max_count rows by resampling with replacement if needed\n",
    "                resampled_subset = subset.sample(n=max_count, replace=True, random_state=42)\n",
    "                \n",
    "                # Reset index and append to resampled data\n",
    "\n",
    "                resampled_data = pd.concat([resampled_data, resampled_subset.reset_index()], axis=0)\n",
    "                logging.info(f\"Resampled data slice {col}_{val} to {max_count} rows with imputation.\")\n",
    "\n",
    "        resampled_data = resampled_data.rename(columns={'index': date_col})\n",
    "        return resampled_data.reset_index(drop=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Data slice created: zone_1 with 30634 rows.\n",
      "INFO:root:Data slice created: zone_7 with 30634 rows.\n",
      "INFO:root:Data slice created: zone_3 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_4 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_5 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_2 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_6 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_0 with 30633 rows.\n",
      "INFO:root:Data slice created: zone_16 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_19 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_24 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_26 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_20 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_22 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_17 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_18 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_25 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_21 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_23 with 34979 rows.\n",
      "INFO:root:Data slice created: zone_8 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_10 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_11 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_12 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_13 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_14 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_15 with 31307 rows.\n",
      "INFO:root:Data slice created: zone_9 with 31307 rows.\n",
      "INFO:root:Mean FeelsLikeF for slice zone_1: 0.53\n",
      "INFO:root:Mean FeelsLikeF for slice zone_7: 0.55\n",
      "INFO:root:Mean FeelsLikeF for slice zone_3: 0.55\n",
      "INFO:root:Mean FeelsLikeF for slice zone_4: 0.55\n",
      "INFO:root:Mean FeelsLikeF for slice zone_5: 0.55\n",
      "INFO:root:Mean FeelsLikeF for slice zone_2: 0.50\n",
      "INFO:root:Mean FeelsLikeF for slice zone_6: 0.54\n",
      "INFO:root:Mean FeelsLikeF for slice zone_0: 0.52\n",
      "INFO:root:Mean FeelsLikeF for slice zone_16: 0.52\n",
      "INFO:root:Mean FeelsLikeF for slice zone_19: 0.49\n",
      "INFO:root:Mean FeelsLikeF for slice zone_24: 0.57\n",
      "INFO:root:Mean FeelsLikeF for slice zone_26: 0.57\n",
      "INFO:root:Mean FeelsLikeF for slice zone_20: 0.50\n",
      "INFO:root:Mean FeelsLikeF for slice zone_22: 0.54\n",
      "INFO:root:Mean FeelsLikeF for slice zone_17: 0.51\n",
      "INFO:root:Mean FeelsLikeF for slice zone_18: 0.51\n",
      "INFO:root:Mean FeelsLikeF for slice zone_25: 0.57\n",
      "INFO:root:Mean FeelsLikeF for slice zone_21: 0.52\n",
      "INFO:root:Mean FeelsLikeF for slice zone_23: 0.55\n",
      "INFO:root:Mean FeelsLikeF for slice zone_8: 0.71\n",
      "INFO:root:Mean FeelsLikeF for slice zone_10: 0.65\n",
      "INFO:root:Mean FeelsLikeF for slice zone_11: 0.68\n",
      "INFO:root:Mean FeelsLikeF for slice zone_12: 0.66\n",
      "INFO:root:Mean FeelsLikeF for slice zone_13: 0.69\n",
      "INFO:root:Mean FeelsLikeF for slice zone_14: 0.74\n",
      "INFO:root:Mean FeelsLikeF for slice zone_15: 0.66\n",
      "INFO:root:Mean FeelsLikeF for slice zone_9: 0.66\n",
      "WARNING:root:Bias detected in slices: ['zone_8', 'zone_14']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting bias for metric column: FeelsLikeF\n",
      "Bias detected in slices: [('zone_8', 0.7086940969340797), ('zone_14', 0.7441896180043099)]\n",
      "Index(['Unnamed: 0', 'precipMM', 'weatherCode', 'visibility', 'HeatIndexF',\n",
      "       'WindChillF', 'windspeedMiles', 'FeelsLikeF', 'tempF_rolling_mean',\n",
      "       'windspeedMiles_rolling_mean', 'humidity_rolling_mean', 'value',\n",
      "       'pressure', 'pressureInches', 'cloudcover', 'uvIndex', 'subba-name',\n",
      "       'zone', 'tempF_rolling_std', 'windspeedMiles_rolling_std',\n",
      "       'humidity_rolling_std', 'tempF_lag_2', 'windspeedMiles_lag_2',\n",
      "       'humidity_lag_2', 'tempF_lag_4', 'windspeedMiles_lag_4',\n",
      "       'humidity_lag_4', 'tempF_lag_6', 'windspeedMiles_lag_6',\n",
      "       'humidity_lag_6', 'month_sin', 'month_cos'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/mlops-project/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'datetime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo significant bias detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m resampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmitigate_bias_resample_with_imputation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslice_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Document the bias report for this metric column\u001b[39;00m\n\u001b[1;32m     44\u001b[0m detector\u001b[38;5;241m.\u001b[39mdocument_bias_report()\n",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m, in \u001b[0;36mDataBiasDetection.mitigate_bias_resample_with_imputation\u001b[0;34m(self, slice_cols, date_col, feature_cols)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     65\u001b[0m resampled_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m---> 66\u001b[0m date_range \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmin(), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[date_col]\u001b[38;5;241m.\u001b[39mmax(), freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m slice_cols:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[col]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# Create a subset for each unique value in the slice column\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlops-project/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/mlops-project/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'datetime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging to see warnings and info\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the dataset from a local CSV file\n",
    "data = pd.read_csv('/Users/akm/Desktop/mlops-project/preprocessed_data.csv')\n",
    "\n",
    "# Instantiate the detector with the loaded data\n",
    "detector = DataBiasDetection(data)\n",
    "\n",
    "# Define columns to slice the data and the metric columns for bias detection\n",
    "slice_columns = ['zone']\n",
    "exclude_columns = ['datetime'] + slice_columns \n",
    "feature_columns = data.columns.difference(exclude_columns)\n",
    "\n",
    "sliced_data = None\n",
    "slice_statistics = None\n",
    "biased_slices = None\n",
    "resampled_data = None\n",
    "\n",
    "# Perform bias detection for each metric column\n",
    "for feature_column in feature_columns:\n",
    "    print(f\"\\nDetecting bias for metric column: {feature_column}\")\n",
    "    \n",
    "    # Slice the data based on the slice columns\n",
    "    sliced_data = detector.data_slicing(slice_cols=slice_columns)\n",
    "    \n",
    "    # Calculate statistics (mean) for each slice based on the metric column\n",
    "    slice_statistics = detector.calculate_statistics(sliced_data, feature_col=feature_column)\n",
    "    \n",
    "    # Detect bias in the slices\n",
    "    biased_slices = detector.detect_bias(slice_statistics)\n",
    "    \n",
    "    # Check and print results\n",
    "    if biased_slices:\n",
    "        print(\"Bias detected in slices:\", biased_slices)\n",
    "    else:\n",
    "        print(\"No significant bias detected.\")\n",
    "    \n",
    "    resampled_data = detector.mitigate_bias_resample_with_imputation(slice_cols=slice_columns, date_col = 'datetime', feature_cols = feature_columns)\n",
    "    # Document the bias report for this metric column\n",
    "    detector.document_bias_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880291, 32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(944433, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
