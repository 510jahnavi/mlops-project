{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "Load pretrained SentenceTransformer: BAAI/bge-small-en\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# system prompt\n",
    "system_prompt = \"You are responsible to answer questions related to energy forcasting and report generation. Keep it concise as possible\"\n",
    "\n",
    "# llm\n",
    "model_name = \"llama3-groq-tool-use\"\n",
    "llm = Ollama(model=model_name, request_timeout=120.0, system=system_prompt)\n",
    "\n",
    "# get embed\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"chromadb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "assistant: The energy demand for Boston is 1,000 MWh. Is there anything else you need assistance with?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=system_prompt\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"Context: boston energy demand is 1000\" + \"| Query: What is the energy demand for boston?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def ingest_documents():\n",
    "    path = \"./documents\"\n",
    "    collection_name = \"documents\"\n",
    "    \n",
    "    reader = SimpleDirectoryReader(input_dir=path)\n",
    "    documents = reader.load_data()\n",
    "    \n",
    "    # get collection\n",
    "    collection = client.get_or_create_collection(collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # ingest and create new index\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, llm=llm, embed_model=embed_model)\n",
    "\n",
    "    return index\n",
    "\n",
    "index = ingest_documents()\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model inference\n",
    "def predict_demand(location_name: str) -> int:\n",
    "    \"\"\"Predicts and returns the energy demand in mwh for the given location name\"\"\"\n",
    "    return 10000\n",
    "\n",
    "def get_weather(location_name: str) -> dict:\n",
    "    \"\"\"Returns the weather dictionary for a given location.\"\"\"\n",
    "    return \"precipiation:100, wind:24, temp:45\"\n",
    "\n",
    "predict_demand_tool = FunctionTool.from_defaults(fn=predict_demand, description=\"function which returns the energy demand given the name of the location\")\n",
    "get_weather_tool = FunctionTool.from_defaults(fn=get_weather, description=\"function which returns the weather given the name of the location\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([predict_demand_tool, get_weather_tool], llm=llm, system_prompt=system_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 48de03f5-6cd6-46fa-80de-6f70fc6b59ec. Step input: What is the weather and energy demand prediction for Boston. Find similar trends from the past data for different subba-names. Use the tools\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: get_weather\n",
      "Action Input: {'location_name': 'Boston'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: precipiation:100, wind:24, temp:45\n",
      "\u001b[0m> Running step c5d853b6-ffa1-4b75-9e40-a7a0cebdfbe4. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: predict_demand\n",
      "Action Input: {'location_name': 'Boston'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 10000\n",
      "\u001b[0m> Running step bd51810e-c72d-42c1-a874-50cb5187687b. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
      "\u001b[0m> Running step 08b89057-ccc0-464e-9788-0c1c4a60b075. Step input: None\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The weather in Boston is precipiation at 100, wind at 24, and temperature at 45. The predicted energy demand is 10,000 units.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the weather and energy demand prediction for Boston. Find similar trends from the past data for different subba-names. Use the tools\")\n",
    "\n",
    "# messages = [\n",
    "#     ChatMessage(\n",
    "#         role=\"system\", content=system_prompt\n",
    "#     ),\n",
    "#     ChatMessage(role=\"user\", content=\"Context: boston energy demand is 1000\" + \"| Query: What is the energy demand for boston?\"),\n",
    "# ]\n",
    "\n",
    "# response = agent.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can summarize that the weather in Boston is currently precipitating with 100% probability, the wind speed is 24 km/h, and the temperature is 45°C. The predicted energy demand for this location is 10,000 units.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
